{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a518eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #activation functions\n",
    "import torch.optim as optim\n",
    "from make_env import make_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea54e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentReplayBuffer:\n",
    "    def __init__(self,max_size,critic_dims,actor_dims,n_actions,n_agents,batch_size):\n",
    "        self.mem_size=max_size\n",
    "        self.mem_cntr=0\n",
    "        self.n_agents=n_agents\n",
    "        self.batch_size=batch_size\n",
    "        self.n_actions=n_actions\n",
    "        self.actor_dims=actor_dims\n",
    "        \n",
    "        self.state_memory=np.zeros((self.mem_size,critic_dims))\n",
    "        self.new_state_memory=np.zeros((self.mem_size,critic_dims))\n",
    "        self.reward_memory=np.zeros((self.mem_size,n_agents))\n",
    "        self.terminal_memory=np.zeros((self.mem_size, n_agents), dtype=bool)\n",
    "        \n",
    "        self.init_actor_memory()\n",
    "    \n",
    "    def init_actor_memory(self):\n",
    "        self.actor_state_memory=[]\n",
    "        self.actor_new_state_memory=[]\n",
    "        self.actor_action_memory=[]\n",
    "        \n",
    "        for i in range(n_agents):\n",
    "            self.actor_state_memory.append(np.zeros((self.mem_size,self.actor_dims[i])))\n",
    "            self.actor_new_state_memory.append(np.zeros((self.mem_size,self.actor_dims[i])))\n",
    "            self.actor_action_memory.append(np.zeros((self.mem_size,self.n_actions)))\n",
    "        \n",
    "        \n",
    "    def store_transition(self,raw_obs,state,action,reward,raw_obs_,state_,done):\n",
    "        if self.mem_cntr % self.mem_size ==0 and self.mem_cntr > 0:\n",
    "            self.init_actor_memory()\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "\n",
    "        for agent_idx in range(self.n_agents):\n",
    "            self.actor_state_memory[agent_idx][index]=raw_obs[agent_idx]\n",
    "            self.actor_new_state_memory[agent_idx][index]=raw_obs[agent_idx]\n",
    "            self.actor_action_memory[agent_idx][index]=action[agent_idx]\n",
    "\n",
    "        self.state_memory[index] =state\n",
    "        self.new_state_memory[index]=state_\n",
    "        self.reward_memory[index]=reward\n",
    "        self.terminal_memory[index]=done\n",
    "\n",
    "        self.mem_cntr +=1\n",
    "\n",
    "    def sample_buffer(self):\n",
    "        max_mem=min(self.mem_cntr,self.mem_size)\n",
    "        batch=np.random.choice(max_mem,self.batch_size,replace=False)\n",
    "\n",
    "        states=self.state_memory[batch]\n",
    "        rewards=self.reward_memory[batch]\n",
    "        state_=self.new_state_memory[batch]\n",
    "        terminal=self.terminal_memory[batch]\n",
    "\n",
    "        actor_states=[]\n",
    "        actor_new_states=[]\n",
    "        actions=[]\n",
    "\n",
    "        for agent_idx in range(self.n_agents):\n",
    "            actor_states.append(self.actor_state_memory[agent_idx][batch])\n",
    "            actor_new_states.append(self.actor_new_state_memory[agent_idx][batch])\n",
    "            actions.append(self.actor_action_memory[agent_idx][batch])\n",
    "\n",
    "        return actor_states,states,actions,rewards,actor_new_states,state_,terminal\n",
    "\n",
    "    def ready(self):\n",
    "        if self.mem_cntr >= self.batch_size:\n",
    "            return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffed43ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self,beta,input_dims,fc1_dims,fc2_dims,n_agents,n_actions,name,chkpt_dir):\n",
    "        super(CriticNetwork,self).__init__()\n",
    "        \n",
    "        self.chkpt_file=os.path.join(chkpt_dir,name)\n",
    "        self.fc1=nn.Linear(input_dims+n_agents*n_actions,fc1_dims)\n",
    "        self.fc2=nn.Linear(fc1_dims,fc2_dims)\n",
    "        self.q=nn.Linear(fc2_dims,1)\n",
    "        self.optimizer=optim.Adam(self.parameters(),lr=beta)\n",
    "        self.device=T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self,state,action):\n",
    "        x=F.relu(self.fc1(T.cat([state,action],dim=1)))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        q=self.q(x)\n",
    "        return q\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(),self.chkpt_file)\n",
    "    \n",
    "    def load_chkpoint(self):\n",
    "        self.load_state_dict(T.load(self.chkpt_file))\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6279fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self,alpha,input_dims,fc1_dims,fc2_dims,n_actions,name,chkpt_dir):\n",
    "        super(ActorNetwork,self).__init__()\n",
    "        \n",
    "        self.chkpt_file=os.path.join(chkpt_dir,name)\n",
    "        self.fc1=nn.Linear(input_dims,fc1_dims)\n",
    "        self.fc2=nn.Linear(fc1_dims,fc2_dims)\n",
    "        self.pi=nn.Linear(fc2_dims,n_actions)\n",
    "        \n",
    "        self.optimizer=optim.Adam(self.parameters(),lr=alpha)\n",
    "        self.device=T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self,state):\n",
    "        x=F.relu(self.fc1(state))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        pi=T.softmax(self.pi(x),dim=1)\n",
    "        \n",
    "        return pi\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(),self.chkpt_file)\n",
    "    \n",
    "    def load_chkpoint(self):\n",
    "        self.load_state_dict(T.load(self.chkpt_file))\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "801f4f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,actor_dims,critic_dims,n_actions,agent_idx,chkpt_dir,alpha=0.01,beta=0.01,fc1=64,fc2=64,gamma=0.95,tau=0.01):\n",
    "        self.gamma=gamma\n",
    "        self.tau=tau\n",
    "        self.n_actions=n_actions\n",
    "        self.agent_name='agent_%s' % agent_idx\n",
    "        \n",
    "        self.actor=ActorNetwork(alpha,actor_dims,fc1,fc2,n_actions,chkpt_dir,self.agent_name+'_actor')\n",
    "        self.critic=CriticNetwork(beta,critic_dims,fc1,fc2,n_agents,n_actions,chkpt_dir,self.agent_name+'_critic')\n",
    "        self.target_actor=ActorNetwork(alpha,actor_dims,fc1,fc2,n_actions,chkpt_dir,self.agent_name+'_target_actor')\n",
    "        self.target_critic=CriticNetwork(beta,critic_dims,fc1,fc2,n_agents,n_actions,chkpt_dir,self.agent_name+'_target_critic')\n",
    "        \n",
    "        self.update_network_parameters(tau=1)\n",
    "    \n",
    "    def update_network_parameters(self,tau=None):\n",
    "        if tau is None:\n",
    "             tau=self.tau\n",
    "                           \n",
    "        target_actor_params=self.target_actor.named_parameters()\n",
    "        actor_params=self.actor.named_parameters()\n",
    "        target_actor_state_dict=dict(target_actor_params)\n",
    "        actor_state_dict=dict(actor_params)\n",
    "        \n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name]=tau*actor_state_dict[name].clone() + (1-tau)*target_actor_state_dict[name].clone()\n",
    "                        \n",
    "        self.target_actor.load_state_dict(actor_state_dict)\n",
    "        target_critic_params=self.target_critic.named_parameters()\n",
    "        critic_params=self.critic.named_parameters()\n",
    "        \n",
    "        target_critic_state_dict=dict(target_critic_params)\n",
    "        critic_state_dict=dict(critic_params)\n",
    "        \n",
    "        for name in critic_state_dict:\n",
    "               critic_state_dict[name]=tau*critic_state_dict[name].clone() + (1-tau)*target_critic_state_dict[name].clone()\n",
    "        self.target_critic.load_state_dict(critic_state_dict)\n",
    "                           \n",
    "    def choose_action(self,observation):\n",
    "        state=T.tensor([observation],dtype=T.float).to(self.actor.device)\n",
    "        actions=self.actor.forward(state)\n",
    "        noise=T.rand(self.n_actions).to(self.actor.device)\n",
    "        action=actions+noise\n",
    "\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "\n",
    "    def save_models(self):\n",
    "        self.actor.save_checkpoint()\n",
    "        self.target.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        self.target_critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        self.target_actor.load_checkpoint()\n",
    "        self.target_critic.load_checkpoint()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf16e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    def __init__(self,actor_dims,critic_dims,n_agents,n_actions,scenario='simple',alpha=0.01,beta=0.01,fc1=64,fc2=64,gamma=0.99,tau=0.01,chkpt_dir='tmp/maddpg/'):\n",
    "        self.agents=[]\n",
    "        self.n_agents=n_agents\n",
    "        self.n_actions=n_actions\n",
    "        chkpt_dir +=scenario\n",
    "        \n",
    "        for agent_idx in range(self.n_agents):\n",
    "            self.agents.append(Agent(actor_dims[agent_idx],critic_dims,n_actions,agent_idx,alpha=alpha,beta=beta,chkpt_dir=chkpt_dir))\n",
    "            \n",
    "    def save_checkpoint(self):\n",
    "        print('..saving checkpoint..')\n",
    "        for agent in self.agents:\n",
    "            agent.save_models()\n",
    "            \n",
    "    def load_checkpoint(self):\n",
    "        print('..loading checkpoint..')\n",
    "        for agent in self.agents:\n",
    "            agent.load_models()\n",
    "            \n",
    "    def choose_action(self,raw_obs):\n",
    "        actions=[]\n",
    "        for agent_idx,agent in enumerate(self.agents):\n",
    "            action=agent.choose_action(raw_obs[agent_idx])\n",
    "            actions.append(action)\n",
    "        return actions\n",
    "    \n",
    "    def learn(self,memory):\n",
    "        if not memory.ready():\n",
    "            return\n",
    "    \n",
    "        actor_states,states,actions,rewards,actor_new_states,states_,dones=memory.sample_buffer()\n",
    "\n",
    "        device=self.agents[0].actor.device\n",
    "        states=T.tensor(states,dtype=T.float).to(device)\n",
    "        actions=T.tensor(actions,dtype=T.float).to(device)\n",
    "        rewards=T.tensor(rewards,dtype=T.float).to(device)\n",
    "        states_=T.tensor(states_,dtype=T.float).to(device)\n",
    "        dones=T.tensor(dones).to(device)\n",
    "        \n",
    "        all_agents_new_actions=[]\n",
    "        all_agents_new_mu_actions=[]\n",
    "        old_agents_actions=[]\n",
    "        \n",
    "        for agent_idx,agent in enumerate(self.agents):\n",
    "            new_states=T.tensor(actor_new_states[agent_idx],dtype=T.float).to(device)\n",
    "            new_pi=agent.target_actor.forward(new_states)\n",
    "            all_agents_new_actions.append(new_pi)\n",
    "            mu_states=T.tensor(actor_states[agent_idx],dtype=T.float).to(device)\n",
    "            pi=agent.actor.forward(mu_states)\n",
    "            all_agents_new_mu_actions.append(pi)\n",
    "            old_agents_actions.append(actions[agent_idx])\n",
    "        \n",
    "        new_actions = T.cat([acts for acts in all_agents_new_actions],dim=1)\n",
    "        mu=T.cat([acts for acts in all_agents_new_mu_actions],dim=1)\n",
    "        old_actions=T.cat([acts for acts in old_agents_actions],dim=1)\n",
    "        \n",
    "        for agent_idx,agent in enumerate(self.agents):\n",
    "            critic_value_=agent.target_critic.forward(states_,new_actions).flatten()\n",
    "            critic_value_[dones[:,0]]=0.0\n",
    "            critic_value=agent.critic.forward(states,old_actions).flatten()\n",
    "            target=rewards[:,agent_idx]+agent.gamma*critic_value_\n",
    "            critic_loss=F.mse_loss(target,critic_value)\n",
    "            agent.critic.optimizer.zero_grad()\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            agent.critic.optimizer.step()\n",
    "            \n",
    "            actor_loss=agent.critic.forward(states,mu).flatten()\n",
    "            actor_loss=-T.mean(actor_loss)\n",
    "            agent.actor.optimizer.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            agent.actor.optimizer.step()\n",
    "            \n",
    "            agent.update_network_parameters()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59027fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7751/3135724898.py:39: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  state=T.tensor([observation],dtype=T.float).to(self.actor.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 500 average_score-68.7\n",
      "episode 1000 average_score-10.4\n",
      "episode 1500 average_score-7.1\n",
      "episode 2000 average_score-7.6\n",
      "episode 2500 average_score-7.4\n",
      "episode 3000 average_score-5.8\n",
      "episode 3500 average_score-7.3\n",
      "episode 4000 average_score-7.5\n",
      "episode 4500 average_score-7.5\n",
      "episode 5000 average_score-7.0\n",
      "episode 5500 average_score-9.0\n",
      "episode 6000 average_score-7.7\n",
      "episode 6500 average_score-9.1\n",
      "episode 7000 average_score-8.8\n",
      "episode 7500 average_score-10.1\n",
      "episode 8000 average_score-8.8\n",
      "episode 8500 average_score-9.8\n",
      "episode 9000 average_score-9.2\n",
      "episode 9500 average_score-8.5\n",
      "episode 10000 average_score-9.7\n",
      "episode 10500 average_score-10.6\n",
      "episode 11000 average_score-7.7\n",
      "episode 11500 average_score-10.7\n",
      "episode 12000 average_score-8.6\n",
      "episode 12500 average_score-8.9\n",
      "episode 13000 average_score-9.3\n",
      "episode 13500 average_score-8.7\n",
      "episode 14000 average_score-7.0\n",
      "episode 14500 average_score-8.9\n",
      "episode 15000 average_score-8.5\n",
      "episode 15500 average_score-6.8\n",
      "episode 16000 average_score-7.4\n",
      "episode 16500 average_score-8.5\n",
      "episode 17000 average_score-8.4\n",
      "episode 17500 average_score-7.1\n",
      "episode 18000 average_score-8.2\n",
      "episode 18500 average_score-8.7\n",
      "episode 19000 average_score-8.6\n",
      "episode 19500 average_score-7.5\n",
      "episode 20000 average_score-7.8\n",
      "episode 20500 average_score-10.2\n",
      "episode 21000 average_score-8.0\n",
      "episode 21500 average_score-7.7\n",
      "episode 22000 average_score-8.3\n",
      "episode 22500 average_score-9.0\n",
      "episode 23000 average_score-8.3\n",
      "episode 23500 average_score-10.2\n",
      "episode 24000 average_score-9.6\n",
      "episode 24500 average_score-8.8\n",
      "episode 25000 average_score-8.2\n",
      "episode 25500 average_score-9.0\n",
      "episode 26000 average_score-8.6\n",
      "episode 26500 average_score-8.2\n",
      "episode 27000 average_score-7.9\n",
      "episode 27500 average_score-8.0\n",
      "episode 28000 average_score-8.0\n",
      "episode 28500 average_score-7.1\n",
      "episode 29000 average_score-9.0\n",
      "episode 29500 average_score-8.0\n"
     ]
    }
   ],
   "source": [
    "           \n",
    "def obs_list_to_state_vector(observation):\n",
    "    state=np.array([])\n",
    "    for obs in observation:\n",
    "        state=np.concatenate([state,obs])\n",
    "    return state\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    scenario='simple'\n",
    "    env=make_env(scenario)\n",
    "    n_agents=env.n\n",
    "    actor_dims=[]\n",
    "    for i in range(n_agents):\n",
    "        actor_dims.append(env.observation_space[i].shape[0])\n",
    "    critic_dims=sum(actor_dims)\n",
    "    n_actions=env.action_space[0].n\n",
    "    maddpg_agents=MADDPG(actor_dims,critic_dims,n_agents,n_actions,fc1=64,fc2=64,alpha=0.01,beta=0.01,scenario=scenario,chkpt_dir='tmp/maddpg/')\n",
    "    memory=MultiAgentReplayBuffer(1000000,critic_dims,actor_dims,n_actions,n_agents,batch_size=1024)\n",
    "\n",
    "    PRINT_INTERVAL=500\n",
    "    N_GAMES=30000\n",
    "    MAX_STEPS=25\n",
    "    total_steps=0\n",
    "    score_history=[]\n",
    "    evaluate=False\n",
    "    best_score=0\n",
    "\n",
    "    if evaluate:\n",
    "        maddpg_agents.load_checkpoint()\n",
    "\n",
    "    for i in range(N_GAMES):\n",
    "        obs=env.reset()\n",
    "        score=0\n",
    "        done=[False]*n_agents\n",
    "        episode_step=0\n",
    "        while not any(done):\n",
    "            if evaluate:\n",
    "                env.render()\n",
    "            actions=maddpg_agents.choose_action(obs)\n",
    "            obs_,reward,done,info=env.step(actions)\n",
    "            state=obs_list_to_state_vector(obs)\n",
    "            state_=obs_list_to_state_vector(obs_)\n",
    "\n",
    "            if episode_step>MAX_STEPS:\n",
    "                done=[True]*n_agents\n",
    "            memory.store_transition(obs,state,actions,reward,obs_,state_,done)\n",
    "\n",
    "            if total_steps % 100 == 0 and not evaluate:\n",
    "                maddpg_agents.learn(memory)\n",
    "\n",
    "            obs=obs_\n",
    "\n",
    "            score+=sum(reward)\n",
    "            total_steps+=1\n",
    "            episode_step+=1\n",
    "\n",
    "        score_history.append(score)\n",
    "        avg_score=np.mean(score_history[-100:])\n",
    "        if not evaluate:\n",
    "            if avg_score > best_score:\n",
    "                maddpg_agents.save_checkpoint()\n",
    "                best_score=avg_score\n",
    "\n",
    "            if i % PRINT_INTERVAL ==0 and i>0:\n",
    "                print('episode',i,'average_score{:.1f}'.format(avg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aeb58d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43920bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de497e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95829960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b011f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c791c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
