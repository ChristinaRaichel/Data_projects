{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet --upgrade nest-asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import io\n",
    "import datetime\n",
    "import collections\n",
    "import functools\n",
    "from typing import List, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_federated as tff\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import (\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    Multiply,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276725</td>\n",
       "      <td>034545104X</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID        ISBN  Rating\n",
       "0   276725  034545104X       0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df = pd.read_csv('data/Ratings.csv',delimiter=';')\n",
    "books_df = pd.read_csv('data/Books.csv',delimiter=';')\n",
    "ratings_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1149780, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = ratings_df['User-ID'].value_counts()\n",
    "ratings_df = ratings_df[ratings_df['User-ID'].isin(counts[counts>60].index)]\n",
    "#ratings_df = ratings_df[ratings_df.Rating > 2]\n",
    "\n",
    "tot_rating_df = ratings_df.groupby(by='ISBN')['Rating'].sum().reset_index().rename(columns={'Rating':'TotRating'})\n",
    "ratings_df = ratings_df.merge(tot_rating_df)\n",
    "ratings_df = ratings_df[ratings_df['TotRating']>20] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>276925</td>\n",
       "      <td>002542730X</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>277427</td>\n",
       "      <td>002542730X</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3363</td>\n",
       "      <td>002542730X</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8019</td>\n",
       "      <td>002542730X</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10030</td>\n",
       "      <td>002542730X</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715085</th>\n",
       "      <td>250925</td>\n",
       "      <td>0836213092</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715086</th>\n",
       "      <td>271284</td>\n",
       "      <td>0836213092</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719995</th>\n",
       "      <td>243294</td>\n",
       "      <td>0006510183</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719996</th>\n",
       "      <td>263344</td>\n",
       "      <td>0006510183</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719997</th>\n",
       "      <td>267033</td>\n",
       "      <td>0006510183</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>278951 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        User-ID        ISBN  Rating\n",
       "8        276925  002542730X      10\n",
       "9        277427  002542730X      10\n",
       "10         3363  002542730X       0\n",
       "11         8019  002542730X       9\n",
       "12        10030  002542730X       7\n",
       "...         ...         ...     ...\n",
       "715085   250925  0836213092      10\n",
       "715086   271284  0836213092       8\n",
       "719995   243294  0006510183      10\n",
       "719996   263344  0006510183       8\n",
       "719997   267033  0006510183       8\n",
       "\n",
       "[278951 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.drop(columns=['TotRating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_mapping = {\n",
    "      old_book: new_book for new_book, old_book in enumerate(\n",
    "          ratings_df.ISBN.unique())\n",
    "  }\n",
    "user_mapping = {\n",
    "    old_user: new_user for new_user, old_user in enumerate(\n",
    "        ratings_df['User-ID'].unique())\n",
    "}\n",
    "\n",
    "ratings_df.ISBN = ratings_df.ISBN.map(book_mapping)\n",
    "ratings_df['User-ID'] = ratings_df['User-ID'].map(user_mapping)\n",
    "books_df.ISBN = books_df.ISBN.map(book_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Rating</th>\n",
       "      <th>TotRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID  ISBN  Rating  TotRating\n",
       "8        0     0      10        297"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_datasets(ratings_df: pd.DataFrame,\n",
    "                       batch_size: int = 1,\n",
    "                       max_examples_per_user: Optional[int] = None,\n",
    "                       max_clients: Optional[int] = None) -> List[tf.data.Dataset]:\n",
    "  \"\"\"Creates TF Datasets containing the movies and ratings for all users.\"\"\"\n",
    "  num_users = len(set(ratings_df['User-ID']))\n",
    "  # Optionally limit to `max_clients` to speed up data loading.\n",
    "  if max_clients is not None:\n",
    "    num_users = min(num_users, max_clients)\n",
    "\n",
    "  def rating_batch_map_fn(rating_batch):\n",
    "    \"\"\"Maps a rating batch to an OrderedDict with tensor values.\"\"\"\n",
    "    # Each example looks like: {x: movie_id, y: rating}.\n",
    "    # We won't need the UserID since each client will only look at their own\n",
    "    # data.\n",
    "    return collections.OrderedDict([\n",
    "        (\"x\", tf.cast(rating_batch[:, 1:2], tf.int64)),\n",
    "        (\"y\", tf.cast(rating_batch[:, 2:3], tf.float32))\n",
    "    ])\n",
    "\n",
    "  tf_datasets = []\n",
    "  for user_id in range(num_users):\n",
    "    # Get subset of ratings_df belonging to a particular user.\n",
    "    user_ratings_df = ratings_df[ratings_df['User-ID'] == user_id]\n",
    "\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices(user_ratings_df)\n",
    "\n",
    "    # Define preprocessing operations.\n",
    "    tf_dataset = tf_dataset.take(max_examples_per_user).shuffle(\n",
    "        buffer_size=max_examples_per_user, seed=42).batch(batch_size).map(\n",
    "        rating_batch_map_fn,\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    tf_datasets.append(tf_dataset)\n",
    "\n",
    "  return tf_datasets\n",
    "\n",
    "\n",
    "def split_tf_datasets(\n",
    "    tf_datasets: List[tf.data.Dataset],\n",
    "    train_fraction: float = 0.8,\n",
    "    val_fraction: float = 0.1,\n",
    ") -> Tuple[List[tf.data.Dataset], List[tf.data.Dataset], List[tf.data.Dataset]]:\n",
    "  \"\"\"Splits a list of user TF datasets into train/val/test by user.\n",
    "  \"\"\"\n",
    "  np.random.seed(42)\n",
    "  np.random.shuffle(tf_datasets)\n",
    "\n",
    "  train_idx = int(len(tf_datasets) * train_fraction)\n",
    "  val_idx = int(len(tf_datasets) * (train_fraction + val_fraction))\n",
    "\n",
    "  # Note that the val and test data contains completely different users, not\n",
    "  # just unseen ratings from train users.\n",
    "  return (tf_datasets[:train_idx], tf_datasets[train_idx:val_idx],\n",
    "          tf_datasets[val_idx:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(federated_train_data[0].as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We limit the number of clients to speed up dataset creation. Feel free to pass\n",
    "# max_clients=None to load all clients' data.\n",
    "tf_datasets = create_tf_datasets(\n",
    "    ratings_df=ratings_df,\n",
    "    batch_size=5,\n",
    "    max_examples_per_user=300,\n",
    "    max_clients=2000)\n",
    "\n",
    "# Split the ratings into training/val/test by client.\n",
    "tf_train_datasets, tf_val_datasets, tf_test_datasets = split_tf_datasets(\n",
    "    tf_datasets,\n",
    "    train_fraction=0.8,\n",
    "    val_fraction=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2868, 13784)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users, n_books = len(ratings_df['User-ID'].unique()), len(ratings_df['ISBN'].unique())\n",
    "n_users, n_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEmbedding(tf.keras.layers.Layer):\n",
    "  \"\"\"Keras layer representing an embedding for a single user, used below.\"\"\"\n",
    "\n",
    "  def __init__(self, num_latent_factors, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.num_latent_factors = num_latent_factors\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.embedding = self.add_weight(\n",
    "        shape=(1, self.num_latent_factors),\n",
    "        initializer='uniform',\n",
    "        dtype=tf.float32,\n",
    "        name='UserEmbeddingKernel')\n",
    "    super().build(input_shape)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return self.embedding\n",
    "\n",
    "  def compute_output_shape(self):\n",
    "    return (1, self.num_latent_factors)\n",
    "\n",
    "\n",
    "def ncf_model(\n",
    "    num_items: int,\n",
    "    num_latent_factors: int,\n",
    "    dense_layers: List[int] = [8, 4],\n",
    "    reg_layers: List[int] = [0.01, 0.01],\n",
    "    activation_dense: str = \"relu\") -> tff.learning.models.ReconstructionModel:\n",
    "  \"\"\"Defines a Keras matrix factorization model.\"\"\"\n",
    "  # Layers with variables will be partitioned into global and local layers.\n",
    "  # We'll pass this to `tff.learning.models.ReconstructionModel.from_keras_model_and_layers`.\n",
    "  global_layers_mf = []\n",
    "  local_layers_mf = []\n",
    "  global_layers_mlp = []\n",
    "  local_layers_mlp = []\n",
    "# Extract the item embedding.\n",
    "  item_input = tf.keras.layers.Input(shape=[1], name='Item')\n",
    "\n",
    "\n",
    "  mf_item_embedding_layer = tf.keras.layers.Embedding(\n",
    "      num_items,\n",
    "      num_latent_factors,\n",
    "      name='mf-ItemEmbedding')\n",
    "      \n",
    "\n",
    "  mlp_item_embedding_layer = tf.keras.layers.Embedding(\n",
    "      num_items,\n",
    "      num_latent_factors,\n",
    "      name='mlp-ItemEmbedding')\n",
    "\n",
    "  global_layers_mf.append(mf_item_embedding_layer)\n",
    "  global_layers_mlp.append(mlp_item_embedding_layer)\n",
    "\n",
    "  flat_item_vec_mf = tf.keras.layers.Flatten(name='FlattenItems-mf')(\n",
    "      mf_item_embedding_layer(item_input))\n",
    "  flat_item_vec_mlp = tf.keras.layers.Flatten(name='FlattenItems-mlp')(\n",
    "      mlp_item_embedding_layer(item_input))\n",
    "  # Extract the user embedding.\n",
    "  mf_user_embedding_layer = UserEmbedding(\n",
    "      num_latent_factors,\n",
    "      name='mf-UserEmbedding')\n",
    "  \n",
    "  mlp_user_embedding_layer = UserEmbedding(\n",
    "      num_latent_factors,\n",
    "      name='mlp-UserEmbedding')\n",
    "  \n",
    "  local_layers_mf.append(mf_user_embedding_layer)\n",
    "  local_layers_mlp.append(mlp_user_embedding_layer)\n",
    "\n",
    "  # The item_input never gets used by the user embedding layer,\n",
    "  # but this allows the model to directly use the user embedding.\n",
    "  flat_user_vec_mf = tf.keras.layers.Flatten()(mf_user_embedding_layer(item_input))\n",
    "  flat_user_vec_mlp = tf.keras.layers.Flatten()(mlp_item_embedding_layer(item_input))\n",
    "  \n",
    "\n",
    "\n",
    "  mf_vector =  Multiply()([flat_user_vec_mf, flat_item_vec_mf])\n",
    "  mlp_vector = Concatenate()([flat_user_vec_mlp, flat_item_vec_mlp])\n",
    "\n",
    "\n",
    "  layer_1 = Dense(\n",
    "          8,\n",
    "          activation=activation_dense,\n",
    "          name = 'layer_1')\n",
    "\n",
    "\n",
    "  mlp_vector = layer_1(mlp_vector)\n",
    "\n",
    "  layer_2 = Dense(\n",
    "          4,\n",
    "          activation=activation_dense,\n",
    "          name = 'layer_2')\n",
    "\n",
    "\n",
    "  mlp_vector = layer_2(mlp_vector)\n",
    "\n",
    "  predict_layer = Concatenate()([mf_vector, mlp_vector])\n",
    "\n",
    "  result = Dense(\n",
    "      1, activation=\"sigmoid\", kernel_initializer=\"lecun_uniform\", name=\"Rating\"\n",
    "  )\n",
    " \n",
    "\n",
    "  output = result(predict_layer)\n",
    "  model = tf.keras.Model(inputs=item_input, outputs=output)\n",
    "  # Compute the dot product between the user embedding, and the item one.\n",
    "  \n",
    "  print(model.trainable_weights)\n",
    "  input_spec = collections.OrderedDict(\n",
    "      x=tf.TensorSpec(shape=[None, 1], dtype=tf.int64),\n",
    "      y=tf.TensorSpec(shape=[None, 1], dtype=tf.float32))\n",
    "\n",
    "  return tff.learning.models.ReconstructionModel.from_keras_model_and_variables(\n",
    "      keras_model=model,\n",
    "      global_trainable_variables=[model.trainable_weights[0],model.trainable_weights[2]],\n",
    "      local_trainable_variables=[model.trainable_weights[1]],\n",
    "      global_non_trainable_variables = [],\n",
    "      local_non_trainable_variables = model.trainable_weights[3:],\n",
    "      input_spec=input_spec)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be used to produce our training process.\n",
    "# User and item embeddings will be 50-dimensional.\n",
    "import functools \n",
    "\n",
    "model_fn = functools.partial(\n",
    "    ncf_model,\n",
    "    num_items=n_books,\n",
    "    num_latent_factors=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingAccuracy(tf.keras.metrics.Mean):\n",
    "\n",
    "\n",
    "  def __init__(self,\n",
    "               name: str = 'rating_accuracy',\n",
    "               **kwargs):\n",
    "    super().__init__(name=name, **kwargs)\n",
    "\n",
    "  def update_state(self,\n",
    "                   y_true: tf.Tensor,\n",
    "                   y_pred: tf.Tensor,\n",
    "                   sample_weight: Optional[tf.Tensor] = None):\n",
    "    absolute_diffs = tf.abs(y_true - y_pred)\n",
    "    # A [batch_size, 1] tf.bool tensor indicating correctness within the\n",
    "    # threshold for each example in a batch. A 0.5 threshold corresponds\n",
    "    # to correctness when predictions are rounded to the nearest whole\n",
    "    # number.\n",
    "    example_accuracies = tf.less_equal(absolute_diffs, 0.5)\n",
    "    super().update_state(example_accuracies, sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "loss_fn = lambda: tf.keras.losses.MeanSquaredError()\n",
    "#metrics_fn = lambda: [RatingAccuracy()]\n",
    "\n",
    "metrics_fn = lambda: [RatingAccuracy(),tfr.keras.metrics.NDCGMetric(name='ndcg_10',topn=10),tfr.keras.metrics.HitsMetric(name='hr_10', topn=10) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_privacy\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def make_optimizer_class(cls):\n",
    "  \"\"\"Constructs a DP optimizer class from an existing one.\"\"\"\n",
    "  parent_code = tf.compat.v1.train.Optimizer.compute_gradients.__code__\n",
    "  child_code = cls.compute_gradients.__code__\n",
    "  GATE_OP = tf.compat.v1.train.Optimizer.GATE_OP  # pylint: disable=invalid-name\n",
    "  if child_code is not parent_code:\n",
    "    logging.warning(\n",
    "        'WARNING: Calling make_optimizer_class() on class %s that overrides '\n",
    "        'method compute_gradients(). Check to ensure that '\n",
    "        'make_optimizer_class() does not interfere with overridden version.',\n",
    "        cls.__name__)\n",
    "\n",
    "  class DPOptimizerClass(cls):\n",
    "    \"\"\"Differentially private subclass of given class cls.\"\"\"\n",
    "\n",
    "    _GlobalState = collections.namedtuple(\n",
    "      '_GlobalState', ['l2_norm_clip', 'stddev'])\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dp_sum_query,\n",
    "        num_microbatches=None,\n",
    "        unroll_microbatches=False,\n",
    "        *args,  # pylint: disable=keyword-arg-before-vararg, g-doc-args\n",
    "        **kwargs):\n",
    "      \"\"\"Initialize the DPOptimizerClass.\n",
    "\n",
    "      Args:\n",
    "        dp_sum_query: DPQuery object, specifying differential privacy\n",
    "          mechanism to use.\n",
    "        num_microbatches: How many microbatches into which the minibatch is\n",
    "          split. If None, will default to the size of the minibatch, and\n",
    "          per-example gradients will be computed.\n",
    "        unroll_microbatches: If true, processes microbatches within a Python\n",
    "          loop instead of a tf.while_loop. Can be used if using a tf.while_loop\n",
    "          raises an exception.\n",
    "      \"\"\"\n",
    "      super(DPOptimizerClass, self).__init__(*args, **kwargs)\n",
    "      self._dp_sum_query = dp_sum_query\n",
    "      self._num_microbatches = num_microbatches\n",
    "      self._global_state = self._dp_sum_query.initial_global_state()\n",
    "      # TODO(b/122613513): Set unroll_microbatches=True to avoid this bug.\n",
    "      # Beware: When num_microbatches is large (>100), enabling this parameter\n",
    "      # may cause an OOM error.\n",
    "      self._unroll_microbatches = unroll_microbatches\n",
    "\n",
    "    def compute_gradients(self,\n",
    "                          loss,\n",
    "                          var_list,\n",
    "                          gate_gradients=GATE_OP,\n",
    "                          aggregation_method=None,\n",
    "                          colocate_gradients_with_ops=False,\n",
    "                          grad_loss=None,\n",
    "                          gradient_tape=None,\n",
    "                          curr_noise_mult=0,\n",
    "                          curr_norm_clip=1):\n",
    "\n",
    "      self._dp_sum_query = gaussian_query.GaussianSumQuery(curr_norm_clip, \n",
    "                                                           curr_norm_clip*curr_noise_mult)\n",
    "      self._global_state = self._dp_sum_query.make_global_state(curr_norm_clip, \n",
    "                                                                curr_norm_clip*curr_noise_mult)\n",
    "      \n",
    "\n",
    "      # TF is running in Eager mode, check we received a vanilla tape.\n",
    "      if not gradient_tape:\n",
    "        raise ValueError('When in Eager mode, a tape needs to be passed.')\n",
    "\n",
    "      vector_loss = loss()\n",
    "      if self._num_microbatches is None:\n",
    "        self._num_microbatches = tf.shape(input=vector_loss)[0]\n",
    "      sample_state = self._dp_sum_query.initial_sample_state(var_list)\n",
    "      microbatches_losses = tf.reshape(vector_loss, [self._num_microbatches, -1])\n",
    "      sample_params = (self._dp_sum_query.derive_sample_params(self._global_state))\n",
    "\n",
    "      def process_microbatch(i, sample_state):\n",
    "        \"\"\"Process one microbatch (record) with privacy helper.\"\"\"\n",
    "        microbatch_loss = tf.reduce_mean(input_tensor=tf.gather(microbatches_losses, [i]))\n",
    "        grads = gradient_tape.gradient(microbatch_loss, var_list)\n",
    "        sample_state = self._dp_sum_query.accumulate_record(sample_params, sample_state, grads)\n",
    "        return sample_state\n",
    "    \n",
    "      for idx in range(self._num_microbatches):\n",
    "        sample_state = process_microbatch(idx, sample_state)\n",
    "\n",
    "      if curr_noise_mult > 0:\n",
    "        grad_sums, self._global_state = (self._dp_sum_query.get_noised_result(sample_state, self._global_state))\n",
    "      else:\n",
    "        grad_sums = sample_state\n",
    "\n",
    "      def normalize(v):\n",
    "        return v / tf.cast(self._num_microbatches, tf.float32)\n",
    "\n",
    "      final_grads = tf.nest.map_structure(normalize, grad_sums)\n",
    "      grads_and_vars = final_grads#list(zip(final_grads, var_list))\n",
    "    \n",
    "      return grads_and_vars\n",
    "\n",
    "  return DPOptimizerClass\n",
    "\n",
    "\n",
    "def make_gaussian_optimizer_class(cls):\n",
    "  \"\"\"Constructs a DP optimizer with Gaussian averaging of updates.\"\"\"\n",
    "\n",
    "  class DPGaussianOptimizerClass(make_optimizer_class(cls)):\n",
    "    \"\"\"DP subclass of given class cls using Gaussian averaging.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        l2_norm_clip=1.5,\n",
    "        noise_multiplier=1.3,\n",
    "        num_microbatches=250,\n",
    "        ledger=None,\n",
    "        unroll_microbatches=False,\n",
    "        *args,  # pylint: disable=keyword-arg-before-vararg\n",
    "        **kwargs):\n",
    "      dp_sum_query = gaussian_query.GaussianSumQuery(\n",
    "          l2_norm_clip, l2_norm_clip * noise_multiplier)\n",
    "\n",
    "      super(DPGaussianOptimizerClass, self).__init__(\n",
    "          dp_sum_query,\n",
    "          num_microbatches,\n",
    "          unroll_microbatches,\n",
    "          *args,\n",
    "          **kwargs)\n",
    "\n",
    "    @property\n",
    "    def ledger(self):\n",
    "      return self._dp_sum_query\n",
    "\n",
    "  return DPGaussianOptimizerClass\n",
    "\n",
    "GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer\n",
    "DPGradientDescentGaussianOptimizer_NEW = make_gaussian_optimizer_class(GradientDescentOptimizer)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"optimizer_tf_dp = tensorflow_privacy.DPKerasSGDOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "resource: Attempting to capture an EagerTensor without building a function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m N_DISC \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# Number of times we train DISC before training GEN once\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Learning Rate for DISCRIMINATOR\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m LR_DISC \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolynomial_decay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_global_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mdecay_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mend_learning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.052\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m BATCH_SIZE \u001b[38;5;241m%\u001b[39m NR_MICROBATCHES \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch size should be an integer multiple of the number of microbatches\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/keras/optimizer_v2/legacy_learning_rate_decay.py:280\u001b[0m, in \u001b[0;36mpolynomial_decay\u001b[0;34m(learning_rate, global_step, decay_steps, end_learning_rate, power, cycle, name)\u001b[0m\n\u001b[1;32m    271\u001b[0m decayed_lr \u001b[38;5;241m=\u001b[39m learning_rate_schedule\u001b[38;5;241m.\u001b[39mPolynomialDecay(\n\u001b[1;32m    272\u001b[0m     learning_rate,\n\u001b[1;32m    273\u001b[0m     decay_steps,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m     cycle\u001b[38;5;241m=\u001b[39mcycle,\n\u001b[1;32m    277\u001b[0m     name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 280\u001b[0m   decayed_lr \u001b[38;5;241m=\u001b[39m \u001b[43mdecayed_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m   decayed_lr \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(decayed_lr, global_step)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:422\u001b[0m, in \u001b[0;36mPolynomialDecay.__call__\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m    419\u001b[0m end_learning_rate \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_learning_rate, dtype)\n\u001b[1;32m    420\u001b[0m power \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpower, dtype)\n\u001b[0;32m--> 422\u001b[0m global_step_recomp \u001b[38;5;241m=\u001b[39m \u001b[43mmath_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m decay_steps_recomp \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecay_steps, dtype)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcycle:\n\u001b[1;32m    425\u001b[0m   \u001b[38;5;66;03m# Find the first multiple of decay_steps that is bigger than\u001b[39;00m\n\u001b[1;32m    426\u001b[0m   \u001b[38;5;66;03m# global_step. If global_step is zero set the multiplier to 1\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:585\u001b[0m, in \u001b[0;36m_EagerTensorBase.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    583\u001b[0m   graph \u001b[38;5;241m=\u001b[39m get_default_graph()\n\u001b[1;32m    584\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mbuilding_function:\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    586\u001b[0m         _add_error_prefix(\n\u001b[1;32m    587\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to capture an EagerTensor without \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilding a function.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    589\u001b[0m             name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    590\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mcapture(\u001b[38;5;28mself\u001b[39m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m__tf_tensor__(dtype, name)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: resource: Attempting to capture an EagerTensor without building a function."
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 60000 # Total size of training data\n",
    "BATCH_SIZE = 600\n",
    "NR_MICROBATCHES = 600 # Each batch of data is split in smaller units called microbatches.\n",
    "\n",
    "\n",
    "NORM_CLIP = 1.1 # Does NOT affect EPSILON, but increases NOISE on gradients\n",
    "NOISE_MULT = 1.15\n",
    "\n",
    "\n",
    "DP_DELTA = 1e-5 # Needs to be smaller than 1/BUFFER_SIZE\n",
    "EPOCHS = 249\n",
    "\n",
    "\n",
    "N_DISC = 1 # Number of times we train DISC before training GEN once\n",
    "\n",
    "\n",
    "# Learning Rate for DISCRIMINATOR\n",
    "LR_DISC = tf.compat.v1.train.polynomial_decay(learning_rate=0.150,\n",
    "                                              global_step=tf.compat.v1.train.get_or_create_global_step(),\n",
    "                                              decay_steps=10000,\n",
    "                                              end_learning_rate=0.052,\n",
    "                                              power=1)\n",
    "\n",
    "if BATCH_SIZE % NR_MICROBATCHES != 0:\n",
    "    raise ValueError('Batch size should be an integer multiple of the number of microbatches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE / NR_MICROBATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer_dp \u001b[38;5;241m=\u001b[39m DPGradientDescentGaussianOptimizer_NEW\n\u001b[1;32m      3\u001b[0m loss_fn_dp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMeanSquaredError(\n\u001b[1;32m      4\u001b[0m      reduction\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mReduction\u001b[38;5;241m.\u001b[39mNONE)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mcompute_dp_sgd_privacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dp_sgd_privacy_statement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber_of_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoise_multiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNOISE_MULT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDP_DELTA\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_privacy/privacy/analysis/compute_dp_sgd_privacy_lib.py:306\u001b[0m, in \u001b[0;36mcompute_dp_sgd_privacy_statement\u001b[0;34m(number_of_examples, batch_size, num_epochs, noise_multiplier, delta, used_microbatching, max_examples_per_user)\u001b[0m\n\u001b[1;32m    296\u001b[0m   paragraph \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mfill(\n\u001b[1;32m    297\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124mExample-level DP with add-or-remove-one adjacency at delta = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m computed \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124mwith RDP accounting:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[1;32m    300\u001b[0m       width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m,\n\u001b[1;32m    301\u001b[0m   )\n\u001b[1;32m    303\u001b[0m   example_eps_no_subsampling \u001b[38;5;241m=\u001b[39m _compute_dp_sgd_example_privacy(\n\u001b[1;32m    304\u001b[0m       num_epochs, noise_multiplier, delta, used_microbatching\n\u001b[1;32m    305\u001b[0m   )\n\u001b[0;32m--> 306\u001b[0m   example_eps_subsampling \u001b[38;5;241m=\u001b[39m \u001b[43m_compute_dp_sgd_example_privacy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnoise_multiplier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m      \u001b[49m\u001b[43mused_microbatching\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpoisson_subsampling_probability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnumber_of_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m   paragraph \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124m    Epsilon with each example occurring once per epoch:  \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mexample_eps_no_subsampling\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m12.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124m    Epsilon assuming Poisson sampling (*):               \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mexample_eps_subsampling\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m12.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    320\u001b[0m   paragraphs\u001b[38;5;241m.\u001b[39mappend(paragraph)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_privacy/privacy/analysis/compute_dp_sgd_privacy_lib.py:233\u001b[0m, in \u001b[0;36m_compute_dp_sgd_example_privacy\u001b[0;34m(num_epochs, noise_multiplier, example_delta, used_microbatching, poisson_subsampling_probability)\u001b[0m\n\u001b[1;32m    228\u001b[0m   count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mceil(num_epochs))\n\u001b[1;32m    229\u001b[0m event_ \u001b[38;5;241m=\u001b[39m dp_accounting\u001b[38;5;241m.\u001b[39mSelfComposedDpEvent(count\u001b[38;5;241m=\u001b[39mcount, event\u001b[38;5;241m=\u001b[39mevent_)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    232\u001b[0m     \u001b[43mdp_accounting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRdpAccountant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;241m.\u001b[39mget_epsilon(example_delta)\n\u001b[1;32m    235\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dp_accounting/privacy_accountant.py:129\u001b[0m, in \u001b[0;36mPrivacyAccountant.compose\u001b[0;34m(self, event, count)\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m UnsupportedEventError(\n\u001b[1;32m    125\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported event: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Error: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    126\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomposition_error\u001b[38;5;241m.\u001b[39merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] caused by subevent \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    127\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomposition_error\u001b[38;5;241m.\u001b[39minvalid_event\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ledger\u001b[38;5;241m.\u001b[39mcompose(event, count)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_compose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dp_accounting/rdp/rdp_privacy_accountant.py:816\u001b[0m, in \u001b[0;36mRdpAccountant._maybe_compose\u001b[0;34m(self, event, count, do_compose)\u001b[0m\n\u001b[1;32m    814\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, dp_event\u001b[38;5;241m.\u001b[39mSelfComposedDpEvent):\n\u001b[0;32m--> 816\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_compose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_compose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, dp_event\u001b[38;5;241m.\u001b[39mComposedDpEvent):\n\u001b[1;32m    818\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m event\u001b[38;5;241m.\u001b[39mevents:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dp_accounting/rdp/rdp_privacy_accountant.py:844\u001b[0m, in \u001b[0;36mRdpAccountant._maybe_compose\u001b[0;34m(self, event, count, do_compose)\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CompositionErrorDetails(\n\u001b[1;32m    838\u001b[0m         invalid_event\u001b[38;5;241m=\u001b[39mevent,\n\u001b[1;32m    839\u001b[0m         error_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubevent of `PoissonSampledDpEvent` must be a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`GaussianDpEvent` or a nested structure of `ComposedDpEvent` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand/or `SelfComposedDpEvent` bottoming out in `GaussianDpEvent`s.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Found subevent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msigma_or_bad_event\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    843\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m do_compose:\n\u001b[0;32m--> 844\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rdp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m count \u001b[38;5;241m*\u001b[39m \u001b[43m_compute_rdp_poisson_subsampled_gaussian\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_probability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoise_multiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma_or_bad_event\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43morders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_orders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, dp_event\u001b[38;5;241m.\u001b[39mSampledWithoutReplacementDpEvent):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dp_accounting/rdp/rdp_privacy_accountant.py:365\u001b[0m, in \u001b[0;36m_compute_rdp_poisson_subsampled_gaussian\u001b[0;34m(q, noise_multiplier, orders)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m alpha \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m noise_multiplier\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    363\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _compute_log_a(q, noise_multiplier, alpha) \u001b[38;5;241m/\u001b[39m (alpha \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([compute_one_order(q, order) \u001b[38;5;28;01mfor\u001b[39;00m order \u001b[38;5;129;01min\u001b[39;00m orders])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dp_accounting/rdp/rdp_privacy_accountant.py:365\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m alpha \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m noise_multiplier\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    363\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _compute_log_a(q, noise_multiplier, alpha) \u001b[38;5;241m/\u001b[39m (alpha \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mcompute_one_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m order \u001b[38;5;129;01min\u001b[39;00m orders])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dp_accounting/rdp/rdp_privacy_accountant.py:363\u001b[0m, in \u001b[0;36m_compute_rdp_poisson_subsampled_gaussian.<locals>.compute_one_order\u001b[0;34m(q, alpha)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m q \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.\u001b[39m:\n\u001b[1;32m    361\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m alpha \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m noise_multiplier\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compute_log_a\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_multiplier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m (alpha \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dp_accounting/rdp/rdp_privacy_accountant.py:335\u001b[0m, in \u001b[0;36m_compute_log_a\u001b[0;34m(q, noise_multiplier, alpha)\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _compute_log_a_int(q, noise_multiplier, \u001b[38;5;28mint\u001b[39m(alpha))\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compute_log_a_frac\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_multiplier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dp_accounting/rdp/rdp_privacy_accountant.py:101\u001b[0m, in \u001b[0;36m_compute_log_a_frac\u001b[0;34m(q, sigma, alpha)\u001b[0m\n\u001b[1;32m     98\u001b[0m log_a0, log_a1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m     99\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 101\u001b[0m z0 \u001b[38;5;241m=\u001b[39m sigma\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m.5\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:  \u001b[38;5;66;03m# do ... until loop\u001b[39;00m\n\u001b[1;32m    104\u001b[0m   coef \u001b[38;5;241m=\u001b[39m special\u001b[38;5;241m.\u001b[39mbinom(alpha, i)\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "optimizer_dp = DPGradientDescentGaussianOptimizer_NEW\n",
    "\n",
    "loss_fn_dp = lambda: tf.keras.losses.MeanSquaredError(\n",
    "     reduction=tf.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy_statement(number_of_examples= 1,batch_size= BATCH_SIZE,\n",
    "    num_epochs= EPOCHS,\n",
    "    noise_multiplier= NOISE_MULT,\n",
    "    delta = DP_DELTA\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'mlp-ItemEmbedding/embeddings:0' shape=(13784, 50) dtype=float32>, <tf.Variable 'mf-UserEmbedding/UserEmbeddingKernel:0' shape=(1, 50) dtype=float32>, <tf.Variable 'mf-ItemEmbedding/embeddings:0' shape=(13784, 50) dtype=float32>, <tf.Variable 'layer_1/kernel:0' shape=(100, 8) dtype=float32>, <tf.Variable 'layer_1/bias:0' shape=(8,) dtype=float32>, <tf.Variable 'layer_2/kernel:0' shape=(8, 4) dtype=float32>, <tf.Variable 'layer_2/bias:0' shape=(4,) dtype=float32>, <tf.Variable 'Rating/kernel:0' shape=(54, 1) dtype=float32>, <tf.Variable 'Rating/bias:0' shape=(1,) dtype=float32>]\n",
      "[<tf.Variable 'mlp-ItemEmbedding/embeddings:0' shape=(13784, 50) dtype=float32>, <tf.Variable 'mf-UserEmbedding/UserEmbeddingKernel:0' shape=(1, 50) dtype=float32>, <tf.Variable 'mf-ItemEmbedding/embeddings:0' shape=(13784, 50) dtype=float32>, <tf.Variable 'layer_1/kernel:0' shape=(100, 8) dtype=float32>, <tf.Variable 'layer_1/bias:0' shape=(8,) dtype=float32>, <tf.Variable 'layer_2/kernel:0' shape=(8, 4) dtype=float32>, <tf.Variable 'layer_2/bias:0' shape=(4,) dtype=float32>, <tf.Variable 'Rating/kernel:0' shape=(54, 1) dtype=float32>, <tf.Variable 'Rating/bias:0' shape=(1,) dtype=float32>]\n",
      "[<tf.Variable 'mlp-ItemEmbedding/embeddings:0' shape=(13784, 50) dtype=float32>, <tf.Variable 'mf-UserEmbedding/UserEmbeddingKernel:0' shape=(1, 50) dtype=float32>, <tf.Variable 'mf-ItemEmbedding/embeddings:0' shape=(13784, 50) dtype=float32>, <tf.Variable 'layer_1/kernel:0' shape=(100, 8) dtype=float32>, <tf.Variable 'layer_1/bias:0' shape=(8,) dtype=float32>, <tf.Variable 'layer_2/kernel:0' shape=(8, 4) dtype=float32>, <tf.Variable 'layer_2/bias:0' shape=(4,) dtype=float32>, <tf.Variable 'Rating/kernel:0' shape=(54, 1) dtype=float32>, <tf.Variable 'Rating/bias:0' shape=(1,) dtype=float32>]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "in user code:\n\n    File \"/home/chris/.local/lib/python3.10/site-packages/tensorflow_federated/python/learning/algorithms/fed_recon.py\", line 208, in client_update  *\n        client_optimizer = keras_optimizer.build_or_verify_tff_optimizer(\n    File \"/home/chris/.local/lib/python3.10/site-packages/tensorflow_federated/python/learning/optimizers/keras_optimizer.py\", line 144, in build_or_verify_tff_optimizer  *\n        optimizer_fn, trainable_weights, disjoint_init_and_next\n    File \"/home/chris/.local/lib/python3.10/site-packages/tensorflow_federated/python/learning/optimizers/keras_optimizer.py\", line 72, in mock_apply_gradients  *\n        opt.apply_gradients(\n    File \"/home/chris/.local/lib/python3.10/site-packages/tensorflow_privacy/privacy/optimizers/dp_optimizer_keras.py\", line 394, in apply_gradients  *\n        assert self._was_dp_gradients_called, (\n\n    AssertionError: Neither _compute_gradients() or get_gradients() on the differentially private optimizer was called. This means the training is not differentially private. It may be the case that you need to upgrade to TF 2.4 or higher to use this particular optimizer.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_process_dp \u001b[38;5;241m=\u001b[39m \u001b[43mtff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_fed_recon\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn_dp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_optimizer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_optimizer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_tf_dp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreconstruction_optimizer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/learning/algorithms/fed_recon.py:531\u001b[0m, in \u001b[0;36mbuild_fed_recon\u001b[0;34m(model_fn, loss_fn, metrics_fn, server_optimizer_fn, client_optimizer_fn, reconstruction_optimizer_fn, dataset_split_fn, client_weighting, model_distributor, model_aggregator_factory, metrics_aggregator)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_split_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    527\u001b[0m   dataset_split_fn \u001b[38;5;241m=\u001b[39m ReconstructionModel\u001b[38;5;241m.\u001b[39mbuild_dataset_split_fn(\n\u001b[1;32m    528\u001b[0m       split_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    529\u001b[0m   )\n\u001b[0;32m--> 531\u001b[0m client_work \u001b[38;5;241m=\u001b[39m \u001b[43m_build_reconstruction_client_work\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_optimizer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_optimizer_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreconstruction_optimizer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreconstruction_optimizer_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_split_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_split_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_weighting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_weighting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_aggregator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_aggregator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m finalizer \u001b[38;5;241m=\u001b[39m apply_optimizer_finalizer\u001b[38;5;241m.\u001b[39mbuild_apply_optimizer_finalizer(\n\u001b[1;32m    543\u001b[0m     optimizer_fn\u001b[38;5;241m=\u001b[39mserver_optimizer_fn, model_weights_type\u001b[38;5;241m=\u001b[39mmodel_weights_type\n\u001b[1;32m    544\u001b[0m )\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m composers\u001b[38;5;241m.\u001b[39mcompose_learning_process(\n\u001b[1;32m    547\u001b[0m     initial_model_weights_fn\u001b[38;5;241m=\u001b[39mbuild_initial_model_weights,\n\u001b[1;32m    548\u001b[0m     model_weights_distributor\u001b[38;5;241m=\u001b[39mmodel_distributor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     model_finalizer\u001b[38;5;241m=\u001b[39mfinalizer,\n\u001b[1;32m    552\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/learning/algorithms/fed_recon.py:172\u001b[0m, in \u001b[0;36m_build_reconstruction_client_work\u001b[0;34m(model_fn, loss_fn, metrics_fn, client_optimizer_fn, reconstruction_optimizer_fn, dataset_split_fn, client_weighting, metrics_aggregator)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Metric finalizer functions that will be populated while tracing\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# `client_update` and used later in the federated computation.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m metric_finalizers: collections\u001b[38;5;241m.\u001b[39mOrderedDict[\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mstr\u001b[39m, metrics_finalizers_lib\u001b[38;5;241m.\u001b[39mKerasMetricFinalizer\n\u001b[1;32m    168\u001b[0m ] \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mOrderedDict()\n\u001b[1;32m    170\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;129;43m@tensorflow_computation\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_computation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_weights_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;129;43m@tf\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\n\u001b[0;32m--> 172\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mclient_update\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minitial_model_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250;43m  \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Performs client local model optimization.\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;43;03m  Args:\u001b[39;49;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;43;03m    A `ClientOutput`.\u001b[39;49;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;43;03m  \"\"\"\u001b[39;49;00m\n\u001b[1;32m    183\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_scope\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py:494\u001b[0m, in \u001b[0;36mComputationWrapper.__call__.<locals>.<lambda>\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    493\u001b[0m   provided_types \u001b[38;5;241m=\u001b[39m _to_types(args)\n\u001b[0;32m--> 494\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m fn: \u001b[43m_wrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovided_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapper_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py:231\u001b[0m, in \u001b[0;36m_wrap\u001b[0;34m(fn, parameter_types, wrapper_fn)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m   \u001b[38;5;66;03m# Either we have a concrete parameter type, or this is no-arg function.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m   parameter_type \u001b[38;5;241m=\u001b[39m _parameter_type(parameters, parameter_types)\n\u001b[0;32m--> 231\u001b[0m   wrapped_fn \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_concrete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrapper_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# When applying a decorator, the __doc__ attribute with the documentation\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# in triple-quotes is not automatically transferred from the function on\u001b[39;00m\n\u001b[1;32m    235\u001b[0m wrapped_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__doc__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py:99\u001b[0m, in \u001b[0;36m_wrap_concrete\u001b[0;34m(fn, wrapper_fn, parameter_type)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m   name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m concrete_fn \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m py_typecheck\u001b[38;5;241m.\u001b[39mcheck_type(\n\u001b[1;32m    101\u001b[0m     concrete_fn,\n\u001b[1;32m    102\u001b[0m     computation_impl\u001b[38;5;241m.\u001b[39mConcreteComputation,\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue returned by the wrapper\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    104\u001b[0m )\n\u001b[1;32m    105\u001b[0m result_parameter_type \u001b[38;5;241m=\u001b[39m concrete_fn\u001b[38;5;241m.\u001b[39mtype_signature\u001b[38;5;241m.\u001b[39mparameter\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/core/environments/tensorflow_frontend/tensorflow_computation.py:66\u001b[0m, in \u001b[0;36m_tf_wrapper_fn\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     63\u001b[0m context_stack \u001b[38;5;241m=\u001b[39m context_stack_impl\u001b[38;5;241m.\u001b[39mcontext_stack\n\u001b[1;32m     64\u001b[0m layout_map \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayout_map\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     65\u001b[0m comp_pb, extra_type_spec \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mtensorflow_serialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_py_fn_as_tf_computation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout_map\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m computation_impl\u001b[38;5;241m.\u001b[39mConcreteComputation(\n\u001b[1;32m     71\u001b[0m     comp_pb,\n\u001b[1;32m     72\u001b[0m     context_stack,\n\u001b[1;32m     73\u001b[0m     extra_type_spec,\n\u001b[1;32m     74\u001b[0m     _transform_result,\n\u001b[1;32m     75\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/core/environments/tensorflow_frontend/tensorflow_serialization.py:111\u001b[0m, in \u001b[0;36mserialize_py_fn_as_tf_computation\u001b[0;34m(fn, parameter_type, context_stack, layout_map)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m variable_utils\u001b[38;5;241m.\u001b[39mrecord_variable_creation_scope() \u001b[38;5;28;01mas\u001b[39;00m all_variables:\n\u001b[1;32m    110\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m parameter_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameter_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/core/impl/computation/function_utils.py:464\u001b[0m, in \u001b[0;36mwrap_as_zero_or_one_arg_callable.<locals>.<lambda>\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArgs to be bound must be in scope.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m arg: \u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/core/impl/computation/function_utils.py:457\u001b[0m, in \u001b[0;36mwrap_as_zero_or_one_arg_callable.<locals>._call\u001b[0;34m(fn, parameter_type, arg, unpack)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(fn, parameter_type, arg, unpack):\n\u001b[1;32m    456\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m unpack_arg(fn, parameter_type, arg, unpack)\n\u001b[0;32m--> 457\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filedzfbmu44.py:78\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__client_update\u001b[0;34m(initial_model_weights, dataset)\u001b[0m\n\u001b[1;32m     76\u001b[0m local_model_weights \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(ReconstructionModel)\u001b[38;5;241m.\u001b[39mget_local_variables, (ag__\u001b[38;5;241m.\u001b[39mld(model),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     77\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure, (ag__\u001b[38;5;241m.\u001b[39mautograph_artifact(\u001b[38;5;28;01mlambda\u001b[39;00m a, b: ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(a)\u001b[38;5;241m.\u001b[39massign, (ag__\u001b[38;5;241m.\u001b[39mld(b),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)), ag__\u001b[38;5;241m.\u001b[39mld(global_model_weights), ag__\u001b[38;5;241m.\u001b[39mld(initial_model_weights)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 78\u001b[0m client_optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeras_optimizer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_or_verify_tff_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_optimizer_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_model_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdisjoint_init_and_next\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m reconstruction_optimizer \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(keras_optimizer)\u001b[38;5;241m.\u001b[39mbuild_or_verify_tff_optimizer, (ag__\u001b[38;5;241m.\u001b[39mld(reconstruction_optimizer_fn), ag__\u001b[38;5;241m.\u001b[39mld(local_model_weights)\u001b[38;5;241m.\u001b[39mtrainable), \u001b[38;5;28mdict\u001b[39m(disjoint_init_and_next\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), fscope)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mfunction\n\u001b[1;32m     82\u001b[0m \u001b[38;5;129m@ag__\u001b[39m\u001b[38;5;241m.\u001b[39mautograph_artifact\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreconstruction_reduce_fn\u001b[39m(state, batch):\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filez5s4dc0_.py:77\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__build_or_verify_tff_optimizer\u001b[0;34m(optimizer_fn, trainable_weights, disjoint_init_and_next)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;167;01mTypeError\u001b[39;00m), (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`optimizer_fn` must be a callable or `tff.learning.optimizers.Optimizer`, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mtype\u001b[39m),\u001b[38;5;250m \u001b[39m(ag__\u001b[38;5;241m.\u001b[39mld(optimizer_fn),),\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;250m \u001b[39mfscope)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     76\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mcallable\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(optimizer_fn),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), if_body, else_body, get_state, set_state, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdo_return\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretval_\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOptimizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdo_return\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretval_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fscope\u001b[38;5;241m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filez5s4dc0_.py:76\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__build_or_verify_tff_optimizer.<locals>.else_body_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m do_return, retval_\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;167;01mTypeError\u001b[39;00m), (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`optimizer_fn` must be a callable or `tff.learning.optimizers.Optimizer`, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mtype\u001b[39m),\u001b[38;5;250m \u001b[39m(ag__\u001b[38;5;241m.\u001b[39mld(optimizer_fn),),\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;250m \u001b[39mfscope)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 76\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdo_return\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretval_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filez5s4dc0_.py:68\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__build_or_verify_tff_optimizer.<locals>.else_body_1.<locals>.if_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(KerasOptimizer), (ag__\u001b[38;5;241m.\u001b[39mld(optimizer_fn), ag__\u001b[38;5;241m.\u001b[39mld(trainable_weights), ag__\u001b[38;5;241m.\u001b[39mld(disjoint_init_and_next)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/learning/optimizers/keras_optimizer.py:79\u001b[0m, in \u001b[0;36mKerasOptimizer.__init__\u001b[0;34m(self, optimizer_fn, weights, disjoint_init_and_next)\u001b[0m\n\u001b[1;32m     72\u001b[0m   opt\u001b[38;5;241m.\u001b[39mapply_gradients(\n\u001b[1;32m     73\u001b[0m       [(tf\u001b[38;5;241m.\u001b[39mzeros_like(w), w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(variables)]\n\u001b[1;32m     74\u001b[0m   )\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Force the creation of tf.Variables controlled by the keras optimizer but\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# keep the variables unmodified. For instance, the \"step\" variable will be\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# 0, not 1, after this operation.\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_apply_gradients\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_files_awgzc_.py:8\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__mock_apply_gradients\u001b[0;34m(opt, variables)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtf__mock_apply_gradients\u001b[39m(opt, variables):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mFunctionScope(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmock_apply_gradients\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfscope\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mConversionOptions(recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, user_requested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, optional_features\u001b[38;5;241m=\u001b[39m(), internal_convert_user_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)) \u001b[38;5;28;01mas\u001b[39;00m fscope:\n\u001b[0;32m----> 8\u001b[0m         \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file_keghftk.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__apply_gradients\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     11\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_was_dp_gradients_called, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeither _compute_gradients() or get_gradients() on the differentially private optimizer was called. This means the training is not differentially private. It may be the case that you need to upgrade to TF 2.4 or higher to use this particular optimizer.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: in user code:\n\n    File \"/home/chris/.local/lib/python3.10/site-packages/tensorflow_federated/python/learning/algorithms/fed_recon.py\", line 208, in client_update  *\n        client_optimizer = keras_optimizer.build_or_verify_tff_optimizer(\n    File \"/home/chris/.local/lib/python3.10/site-packages/tensorflow_federated/python/learning/optimizers/keras_optimizer.py\", line 144, in build_or_verify_tff_optimizer  *\n        optimizer_fn, trainable_weights, disjoint_init_and_next\n    File \"/home/chris/.local/lib/python3.10/site-packages/tensorflow_federated/python/learning/optimizers/keras_optimizer.py\", line 72, in mock_apply_gradients  *\n        opt.apply_gradients(\n    File \"/home/chris/.local/lib/python3.10/site-packages/tensorflow_privacy/privacy/optimizers/dp_optimizer_keras.py\", line 394, in apply_gradients  *\n        assert self._was_dp_gradients_called, (\n\n    AssertionError: Neither _compute_gradients() or get_gradients() on the differentially private optimizer was called. This means the training is not differentially private. It may be the case that you need to upgrade to TF 2.4 or higher to use this particular optimizer.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_process_dp = tff.learning.algorithms.build_fed_recon(\n",
    "    model_fn=model_fn,\n",
    "    loss_fn=loss_fn_dp,\n",
    "    metrics_fn=metrics_fn,\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.5),\n",
    "    client_optimizer_fn=lambda: optimizer_dp,\n",
    "    reconstruction_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.001))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
